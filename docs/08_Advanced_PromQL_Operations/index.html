<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Lab 8: Advanced PromQL Operations</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/night.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../common.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section data-markdown>
                <textarea data-template>
                    # Lab 8: Advanced PromQL Operations

                    üîç Advanced PromQL
                    
                    <small>Navigate: <a href="../index.html">All Slides</a></small>
                    
<aside class="notes">
Welcome to Lab 8, where we'll explore some of the most powerful and advanced features of PromQL!

Throughout our previous labs, we've built a strong foundation in PromQL, learning how to query metrics, calculate rates, aggregate data, and create alerts. Today, we're taking our skills to the next level with techniques that will allow you to perform sophisticated analyses and transformations on your metrics.

The operations we'll cover in this lab are what separates casual PromQL users from true experts. These techniques enable you to reshape data, perform complex time-based analyses, and extract insights that would be difficult or impossible with basic queries.

By the end of this lab, you'll have a comprehensive toolkit of advanced PromQL techniques that you can apply to real-world monitoring challenges. Whether you're troubleshooting performance issues, analyzing trends, or building sophisticated dashboards, the skills you learn today will significantly enhance your monitoring capabilities.

Let's dive into these advanced PromQL operations and unlock the full potential of Prometheus for your monitoring needs.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Objectives
                    
                    * Learn how to use label manipulation functions
                    * Understand the offset modifier for historical comparisons
                    * Master subqueries for complex time-based analysis
                    * Use topk and bottomk functions for value ranking
                    * Apply the absent function to detect missing metrics
                    
<aside class="notes">
In this lab, we'll focus on five key advanced PromQL techniques:

First, we'll explore label manipulation functions, which allow you to modify, add, or combine metric labels at query time. This is incredibly powerful for categorizing and organizing your metrics in ways that weren't defined when the metrics were collected.

Next, we'll look at the offset modifier, which enables historical comparisons by letting you query data from specific points in the past. This is essential for comparing current metrics to previous baselines or identifying trends over time.

We'll then dive into subqueries, one of PromQL's most powerful features. Subqueries let you apply functions to the results of range queries over time windows, enabling complex analyses like moving averages, trend detection, and anomaly spotting.

The topk and bottomk functions will be our next focus. These ranking functions help you identify outliers by selecting the highest or lowest values in a set of time series. They're invaluable for finding resource hogs or underutilized resources in your systems.

Finally, we'll learn about the absent function, which detects missing metrics. This is crucial for monitoring the health of your monitoring system itself and ensuring that expected metrics are actually being collected.

Each of these techniques addresses specific monitoring challenges that you're likely to encounter in real-world scenarios. By mastering them, you'll be able to extract more valuable insights from your metrics and build more comprehensive monitoring solutions.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Label Manipulation Functions
                    
                    Add, modify, or join labels at query time without changing the underlying data
                    
<aside class="notes">
Label manipulation functions are powerful tools that allow you to transform metric labels at query time, without modifying the underlying data stored in Prometheus.

In Prometheus's data model, labels are key-value pairs attached to metrics that provide additional dimensions for filtering and grouping. While labels are typically defined when metrics are collected, there are many scenarios where you might want to modify these labels during analysis.

For example, you might want to:
- Add new labels to categorize metrics (e.g., adding "tier" labels to distinguish between production and staging instances)
- Create composite labels by combining existing ones (e.g., creating a "service-instance" label from separate "service" and "instance" labels)
- Transform label values to standardize formats or correct inconsistencies
- Extract portions of label values using regular expressions

Prometheus provides two main functions for label manipulation: label_replace and label_join. These functions are incredibly versatile and can significantly enhance your ability to organize and analyze metrics in meaningful ways.

The key advantage of label manipulation is that it happens at query time, giving you flexibility to reshape your metric data on-the-fly for specific analysis needs without having to reconfigure your exporters or instrumentation.

Let's look at each of these functions in detail.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## label_replace Function
                    
                    ```promql
                    label_replace(
                      node_filesystem_size_bytes{instance="localhost:9100",mountpoint="/"},
                      "disk_type",
                      "root_disk",
                      "mountpoint",
                      "/"
                    )
                    ```
                    
                    * Adds a new label `disk_type` with value `root_disk`
                    * For metrics where `mountpoint` matches pattern `/`
                    * Format: `label_replace(v, dst_label, replacement, src_label, regex)`
                    * Useful for categorizing or grouping metrics
                    
<aside class="notes">
The label_replace function is one of the most versatile label manipulation tools in PromQL. It allows you to add or replace labels based on pattern matching against existing label values.

Let's break down the example shown:

1. We start with the metric `node_filesystem_size_bytes`, filtering for a specific instance and mountpoint.
2. We add a new label called `disk_type` with the value `root_disk`.
3. This happens only where the `mountpoint` label matches the pattern `/`.

The function takes five arguments:
- The vector to transform (our filesystem metric)
- The destination label name to create or replace (`disk_type`)
- The replacement text for the new label value (`root_disk`)
- The source label to match against (`mountpoint`)
- A regular expression pattern to match (`/`)

This particular example is simple, but label_replace becomes even more powerful when used with regular expressions that extract portions of existing labels. For example, you could extract the domain from a URL, or parse components from a complex identifier.

A more advanced example might be:

```
label_replace(
  some_metric{instance=~".*"},
  "datacenter",
  "$1",
  "instance",
  "(us|eu|ap)-.*"
)
```

This would extract the datacenter region (us, eu, or ap) from the instance name and create a new `datacenter` label with that value.

Label_replace is particularly useful for:
- Adding classification labels for better organization
- Standardizing inconsistent labels across different sources
- Extracting meaningful components from complex identifiers
- Creating new dimensions for aggregation or filtering

When teaching this concept, it's helpful to have students experiment with different regex patterns to see how they can transform their data in various ways.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## label_join Function
                    
                    ```promql
                    label_join(
                      node_filesystem_size_bytes{instance="localhost:9100",mountpoint="/"},
                      "instance_path",
                      "-",
                      "instance",
                      "mountpoint"
                    )
                    ```
                    
                    * Creates new label `instance_path` by joining values
                    * Joins `instance` and `mountpoint` with `-` separator
                    * Format: `label_join(v, dst_label, separator, src_label1, src_label2, ...)`
                    * Useful for creating composite identifiers
                    
<aside class="notes">
While label_replace is great for pattern matching and extraction, label_join serves a different purpose: combining values from multiple labels into a single new label.

Looking at our example:

1. We start with the same `node_filesystem_size_bytes` metric.
2. We create a new label called `instance_path`.
3. This new label combines the values of the `instance` and `mountpoint` labels, separated by a hyphen.

The function takes these arguments:
- The vector to transform (our filesystem metric)
- The destination label to create (`instance_path`)
- The separator string (`-`)
- Two or more source labels to join (`instance` and `mountpoint`)

The result would be a metric with a new label like `instance_path="localhost:9100-/"`.

Label_join is particularly useful for:
- Creating composite identifiers that make metrics more descriptive
- Combining dimensions for more specific grouping
- Forming hierarchical labels from separate components
- Making labels that are more human-readable or meaningful for reporting

A practical use case might be combining service and environment labels to create service-specific dashboards that clearly show which environment each metric comes from.

One important note: both label_replace and label_join create new labels without removing the original ones. This means you retain all the original dimensions for filtering, while gaining new ones for different analysis perspectives.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## The offset Modifier
                    
                    ```promql
                    # CPU usage now vs. 1 hour ago
                    rate(node_cpu_seconds_total{mode="user"}[5m])
                    vs
                    rate(node_cpu_seconds_total{mode="user"}[5m] offset 1h)
                    ```
                    
                    * Looks back in time to query historical data
                    * Allows comparing current vs. past metrics
                    * Can be used with instant or range vectors
                    * Useful for:
                      * Detecting trends
                      * Comparing to baselines
                      * Calculating week-over-week changes
                    
<aside class="notes">
The offset modifier is one of the most useful time-manipulation features in PromQL. It allows you to look backward in time to query historical data from a specific point in the past.

In our example, we're comparing two queries:
1. The current CPU usage rate calculated over the last 5 minutes
2. The CPU usage rate from exactly 1 hour ago, also calculated over a 5-minute window

The syntax is straightforward: just add `offset <time>` after your time range selector. The offset value can use any of Prometheus's time units: s (seconds), m (minutes), h (hours), d (days), w (weeks), y (years).

This capability is incredibly powerful for several reasons:

First, it allows you to directly compare current metrics with historical baselines. For example, you could create a query that divides current values by values from a week ago to get a percentage change.

Second, it enables you to implement patterns like "week-over-week" or "day-over-day" comparisons, which are essential for identifying seasonal patterns or unusual deviations from historical norms.

Third, it gives you the ability to create alerts based not just on absolute thresholds, but on relative changes from historical values. This can make your alerting more adaptive to the normal patterns of your systems.

Some practical applications include:
- Comparing today's error rate with yesterday's at the same time of day
- Alerting when traffic drops more than 20% compared to the same time last week
- Visualizing month-over-month growth in resource consumption

One important note: the offset modifier applies to the time selection, not to the evaluation time of the entire expression. This allows you to combine current and historical data in the same query.

For example, you could write:
```
rate(http_requests_total[5m]) / rate(http_requests_total[5m] offset 1d) > 1.5
```

This would alert if today's request rate is 50% higher than it was at the same time yesterday.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Offset Use Cases
                    
                    ```promql
                    # Day-over-day comparison (percentage change)
                    100 * (
                      rate(http_requests_total[5m]) /
                      rate(http_requests_total[5m] offset 1d) - 1
                    )
                    
                    # Week-over-week comparison
                    rate(node_cpu_seconds_total{mode="user"}[5m]) -
                    rate(node_cpu_seconds_total{mode="user"}[5m] offset 7d)
                    ```
                    
                    * Compare to same time yesterday, last week, etc.
                    * Calculate growth or decline over specific periods
                    * Great for business metrics and capacity planning
                    
<aside class="notes">
Let's explore some concrete use cases for the offset modifier that demonstrate its practical value in monitoring and analysis.

Our first example shows how to calculate a percentage change between current metrics and metrics from the same time yesterday:

1. We take the current request rate over the last 5 minutes
2. Divide it by the request rate from exactly 24 hours ago
3. Subtract 1 to get the relative change (1.2 would be 20% increase)
4. Multiply by 100 to convert to a percentage

This pattern is extremely useful for business metrics, where day-over-day or week-over-week comparisons are common. For example, an e-commerce site might want to compare today's order rate with the same day last week, or a SaaS application might track user engagement relative to previous periods.

The second example shows an absolute difference calculation:
1. We calculate the current CPU usage rate
2. Subtract the CPU usage rate from exactly 7 days ago
3. The result shows how much more (or less) CPU is being used compared to last week

This type of comparison is valuable for capacity planning and trend analysis. By looking at how resource usage is changing over time, you can project future needs and address potential constraints before they become problems.

Additional use cases for offset include:
- Anomaly detection: Flag metrics that deviate significantly from historical patterns
- Seasonal adjustments: Account for expected variations in metrics based on time of day, day of week, etc.
- SLO tracking: Compare current error rates or latencies against historical baselines

One advanced technique is to use multiple offsets to create a "seasonal" baseline - for example, averaging the metrics from the same time period over several previous weeks to establish a more robust comparison point.

When working with offsets, remember that data retention limits in your Prometheus server will determine how far back you can query. If you need very long-term comparisons, you might need to adjust your storage settings or use recording rules to preserve historical values.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Subqueries
                    
                    ```promql
                    max_over_time(rate(node_cpu_seconds_total{mode="user"}[5m])[1h:1m])
                    ```
                    
                    * Format: `<instant_query>[<range>:<step>]`
                    * Evaluates the inner query over a range with a specific resolution
                    * Allows applying functions to ranges of already-processed data
                    * Critical for advanced time-series analysis
                    
<aside class="notes">
Subqueries are one of the most powerful but also complex features in PromQL. They allow you to apply range functions to the results of other queries over time, enabling sophisticated time-series analysis that would otherwise be impossible.

Let's break down the example:

1. The inner query `rate(node_cpu_seconds_total{mode="user"}[5m])` calculates the per-second rate of CPU usage over 5-minute windows.
2. The subquery syntax `[1h:1m]` evaluates this inner query over the last hour, at 1-minute intervals.
3. Finally, `max_over_time` finds the maximum value in each of these time series over the 1-hour range.

The result is the maximum CPU usage rate observed in each time series during the past hour, which helps identify peak resource utilization.

The general format for subqueries is:
`<instant_query>[<range>:<step>]`

Where:
- `<instant_query>` is any PromQL expression that returns an instant vector
- `<range>` is how far back to evaluate the query (e.g., 1h, 24h)
- `<step>` is the evaluation interval or resolution (e.g., 1m, 5m)

Subqueries are especially useful when combined with these range vector functions:
- `avg_over_time`: Calculate moving averages
- `max_over_time`: Find peak values over periods
- `min_over_time`: Find minimum values over periods
- `quantile_over_time`: Calculate percentiles over periods
- `stddev_over_time`: Measure variation/volatility

Some practical applications include:
- Calculating the 95th percentile of request latencies over the past day
- Finding the maximum error rate observed in 5-minute windows throughout the day
- Computing a moving average of resource usage to smooth out spikes

One important consideration with subqueries is their performance impact. Because they evaluate the inner query many times (once for each step within the range), they can be resource-intensive. Use them judiciously and consider using recording rules for frequently used subqueries.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Subquery Examples
                    
                    ```promql
                    # Moving average of request rate over 30 minutes
                    avg_over_time(rate(http_requests_total[5m])[30m:1m])
                    
                    # Max memory usage by instance over the last 6 hours
                    max_over_time(
                      (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
                      / node_memory_MemTotal_bytes
                      [6h:5m]
                    )
                    ```
                    
                    * Powerful for trend analysis and anomaly detection
                    * Enable patterns like moving averages and percentiles
                    * Help smooth out noisy metrics
                    
<aside class="notes">
Let's explore some more practical examples of subqueries to better understand their applications in real-world monitoring scenarios.

Our first example demonstrates how to calculate a moving average:

```promql
avg_over_time(rate(http_requests_total[5m])[30m:1m])
```

This query:
1. Calculates the request rate over 5-minute windows
2. Evaluates this rate every minute for the past 30 minutes
3. Computes the average of these rates for each time series

The result is a smoothed version of your request rate that filters out short-term spikes and fluctuations. This is extremely valuable for noisy metrics where the overall trend is more important than momentary variations.

The second example shows how to track peak resource usage:

```promql
max_over_time(
  (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
  / node_memory_MemTotal_bytes
  [6h:5m]
)
```

This query:
1. Calculates memory usage as a fraction of total memory
2. Evaluates this calculation every 5 minutes for the past 6 hours
3. Returns the maximum value observed during that period for each instance

This approach helps you identify peak memory utilization, which is critical for capacity planning and identifying potential resource constraints.

Other valuable applications of subqueries include:

- **Anomaly detection**: Compare current values to historical ranges using expressions like:
  ```promql
  rate(errors_total[5m]) > 2 * avg_over_time(rate(errors_total[5m])[1d:1h])
  ```
  This alerts when the current error rate exceeds twice the average rate over the past day.

- **Percentile tracking over time**: Monitor how your 95th percentile latency has changed:
  ```promql
  quantile_over_time(0.95, http_request_duration_seconds[5m])[1d:1h]
  ```

- **Volatility measurement**: Track how stable or variable a metric is:
  ```promql
  stddev_over_time(rate(http_requests_total[5m])[1h:1m]) / avg_over_time(rate(http_requests_total[5m])[1h:1m])
  ```
  This calculates the coefficient of variation, a normalized measure of variability.

When teaching subqueries, it's helpful to visualize the results alongside the raw data to show how techniques like moving averages smooth out noise while preserving the underlying trends.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## topk and bottomk Functions
                    
                    ```promql
                    # Find the 3 instances with highest CPU usage
                    topk(3, sum by(instance) (rate(node_cpu_seconds_total{mode!="idle"}[5m])))
                    
                    # Find the 5 containers with lowest memory usage
                    bottomk(5, container_memory_usage_bytes / container_spec_memory_limit_bytes)
                    ```
                    
                    * Select the highest (topk) or lowest (bottomk) N elements
                    * Format: `topk/bottomk(N, instant-vector)`
                    * Great for identifying outliers
                    * Useful for dashboards and troubleshooting
                    
<aside class="notes">
The topk and bottomk functions are ranking operators that help you identify outliers in your metrics - either the highest or lowest values across a set of time series.

These functions are particularly useful when you have many instances, services, or endpoints and want to quickly identify which ones are consuming the most resources or experiencing issues.

Let's examine the examples:

The first query uses topk to find the three instances with the highest CPU usage:
1. We calculate the CPU usage rate for all modes except idle
2. We sum these rates by instance to get total CPU usage per instance
3. The topk(3, ...) function selects the three instances with the highest values

This is extremely useful for quickly identifying which servers or services are under the heaviest load, which might be causing performance issues or require scaling.

The second query uses bottomk to find the five containers with the lowest memory utilization relative to their limits:
1. We divide actual memory usage by the memory limit for each container
2. The bottomk(5, ...) function selects the five containers with the lowest values

This might help identify underutilized resources that could be scaled down to save costs.

Some important points about these functions:

- Both functions take exactly two arguments: a number N and an instant vector
- They return exactly N elements, unless there are fewer than N elements in the input vector
- If there are ties (multiple time series with identical values), the behavior is undefined
- The result includes both the metric values and all their labels, making it easy to identify specific instances

Practical applications include:
- Identifying the busiest API endpoints for optimization
- Finding the services with the highest error rates for troubleshooting
- Determining which databases have the most connections or highest query latency
- Creating "top N" dashboard panels that focus attention on potential problem areas

These functions are invaluable for focusing your attention on the most important metrics among potentially hundreds or thousands of time series.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## topk/bottomk Examples
                    
                    ```promql
                    # Top 10 HTTP endpoints by 95th percentile latency
                    topk(10, 
                      histogram_quantile(0.95, 
                        sum by(le, handler) (rate(http_request_duration_seconds_bucket[5m]))
                      )
                    )
                    
                    # Hosts with the 5 lowest disk space available
                    bottomk(5, 
                      node_filesystem_avail_bytes{mountpoint="/"} / 
                      node_filesystem_size_bytes{mountpoint="/"}
                    )
                    ```
                    
                    * Can be combined with other PromQL features
                    * Help focus on what matters most
                    * Great for operational dashboards
                    
<aside class="notes">
Let's explore some more complex examples of topk and bottomk that demonstrate how they can be combined with other PromQL features to create powerful insights.

Our first example shows how to identify the slowest HTTP endpoints:

```promql
topk(10, 
  histogram_quantile(0.95, 
    sum by(le, handler) (rate(http_request_duration_seconds_bucket[5m]))
  )
)
```

This query combines several PromQL concepts:
1. It uses `rate` to calculate the per-second increase in histogram buckets
2. It aggregates these rates by bucket limit (`le`) and endpoint handler
3. It calculates the 95th percentile latency for each handler using `histogram_quantile`
4. Finally, it uses `topk` to select the 10 endpoints with the highest 95th percentile latencies

This is extremely valuable for identifying which API endpoints or pages are providing the poorest user experience and should be prioritized for optimization.

The second example finds hosts that are running low on disk space:

```promql
bottomk(5, 
  node_filesystem_avail_bytes{mountpoint="/"} / 
  node_filesystem_size_bytes{mountpoint="/"}
)
```

This query:
1. Calculates the fraction of available disk space for each root filesystem
2. Uses `bottomk` to select the 5 hosts with the lowest available space ratio

This helps you proactively address storage issues before they become critical.

Some other practical applications of these ranking functions include:

- **Capacity planning dashboards**: Show the top N hosts by resource utilization to identify scaling needs
  ```promql
  topk(5, sum by(instance) (rate(node_cpu_seconds_total{mode!="idle"}[5m])) / on(instance) count by(instance) (node_cpu_seconds_total{mode="idle"}))
  ```

- **Cost optimization**: Identify underutilized resources that might be candidates for downsizing
  ```promql
  bottomk(10, avg_over_time(node_cpu_seconds_total{mode="user"}[7d]) / on(instance) count by(instance) (node_cpu_seconds_total{mode="idle"}))
  ```

- **Error tracking**: Find services with the highest error rates
  ```promql
  topk(5, sum by(service) (rate(http_requests_total{status_code=~"5.."}[5m])) / sum by(service) (rate(http_requests_total[5m])))
  ```

In operational dashboards, these functions are often used to create "Top N" panels that immediately draw attention to outliers that might require investigation. They help focus attention on what matters most when monitoring large-scale systems with thousands of metrics.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## The absent Function
                    
                    ```promql
                    # Will return a value of 1 if the metric is missing
                    absent(up{job="node-exporter"})
                    
                    # Combined with alert: trigger if metric is missing
                    absent(node_exporter_build_info)
                    ```
                    
                    * Returns 1 if the metric doesn't exist, otherwise returns nothing
                    * Useful for detecting:
                      * Missing exporters
                      * Services that stopped reporting
                      * Configuration errors
                    * Critical for monitoring your monitoring
                    
<aside class="notes">
The absent function is a unique operator in PromQL that helps you detect when metrics are missing entirely. This is distinct from a metric having a value of 0 - it specifically checks for the absence of time series that should exist.

This capability is critical for what we call "monitoring your monitoring" - ensuring that your instrumentation itself is working correctly.

Let's examine how it works:

1. If the time series specified in the argument doesn't exist, the function returns a time series with a value of 1 and no labels (or with the same external labels that were provided in the argument).
2. If the time series does exist, the function returns no time series at all (an empty result).

This behavior seems unusual at first, but it's designed specifically to work well with Prometheus's alerting system:

In the first example, `absent(up{job="node-exporter"})`, we're checking if the up metric for the node-exporter job is missing. If the node exporter is running and reporting metrics properly, this expression returns nothing. But if the exporter stops reporting entirely, it returns a value of 1, which can trigger an alert.

The second example, `absent(node_exporter_build_info)`, checks if the build information metric for the node exporter exists at all. This is useful for verifying that specific metrics you expect to be collected are actually present.

Some important use cases for absent include:

- Detecting when exporters or services stop reporting metrics entirely
- Verifying that specific metrics are being collected as expected
- Identifying configuration errors in your monitoring setup
- Creating "deadman switch" alerts that trigger if monitoring stops functioning

For example, you might create an alert rule like:

```
alert: NodeExporterMissing
expr: absent(up{job="node-exporter"})
for: 5m
labels:
  severity: critical
annotations:
  summary: "Node Exporter missing"
  description: "Node exporter has disappeared from Prometheus target discovery."
```

This would fire if the node exporter's up metric disappears completely for 5 minutes, which likely indicates a serious problem with either the exporter or with Prometheus's ability to scrape it.

The absent function is an essential tool for ensuring the reliability of your monitoring infrastructure itself.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## absent Examples
                    
                    ```promql
                    # Check if any instances are missing for a job
                    absent(sum by (instance) (up{job="api-server"}))
                    
                    # Alert if a critical business metric disappears
                    absent(orders_processed_total)
                    
                    # Combined with or to alert on specific instances
                    up{job="node-exporter"} == 0 or absent(up{job="node-exporter"})
                    ```
                    
                    * Can be combined with other operators
                    * Essential for creating reliable alerts
                    * Helps ensure monitoring system integrity
                    
<aside class="notes">
Let's explore some more nuanced examples of how the absent function can be used in real-world monitoring scenarios.

Our first example demonstrates how to check if any expected instances are missing for a specific job:

```promql
absent(sum by (instance) (up{job="api-server"}))
```

This query:
1. Aggregates the up metric by instance for the api-server job
2. Checks if this aggregated metric is missing entirely

This would return a value if no instances of the api-server job are being monitored at all, which might indicate a configuration issue or a complete service outage.

The second example focuses on business metrics rather than infrastructure metrics:

```promql
absent(orders_processed_total)
```

This checks if a critical business metric (orders_processed_total) is being reported. The absence of this metric might indicate an issue with your instrumentation or a serious problem with the order processing system itself.

The third example shows how to combine absent with other operators for more comprehensive checks:

```promql
up{job="node-exporter"} == 0 or absent(up{job="node-exporter"})
```

This expression will trigger both if:
1. The node-exporter is reporting but its up metric is 0 (indicating it's running but unhealthy)
2. The node-exporter isn't reporting any metrics at all (complete failure)

This provides more comprehensive coverage than checking either condition alone.

Some advanced patterns with absent include:

- **Dynamic service discovery checks**: When using service discovery, ensure expected services are being found:
  ```promql
  absent(count by(env) (up{job="api-server", env="production"}))
  ```
  This alerts if no production API servers are discovered.

- **Expected metric cardinality**: Check if you're losing label dimensions:
  ```promql
  absent(count by(method) (http_requests_total{job="api-server"}))
  ```
  This alerts if method labels disappear from your HTTP metrics.

- **Staleness checks**: Combined with timestamp functions:
  ```promql
  absent(http_requests_total) or (time() - timestamp(http_requests_total)) > 300
  ```
  This alerts if the metric is either missing or hasn't been updated in 5 minutes.

When implementing these checks, it's important to set appropriate thresholds for metric staleness or absence. A metric might legitimately disappear briefly during deployments or maintenance, so include sufficient delay periods in your alerts to avoid false positives.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Putting It All Together
                    
                    ```promql
                    # Complex query combining multiple techniques
                    topk(5,
                      sum by (instance) (
                        rate(node_cpu_seconds_total{mode!="idle"}[5m])
                      ) / 
                      on(instance) (
                        count by(instance) (node_cpu_seconds_total{mode="idle"})
                      ) > 
                      1.2 * 
                      avg_over_time(
                        sum by (instance) (
                          rate(node_cpu_seconds_total{mode!="idle"}[5m])
                        ) / 
                        on(instance) (
                          count by(instance) (node_cpu_seconds_total{mode="idle"})
                        )[1d:1h]
                      )
                    )
                    ```
                    
                    * Advanced queries can combine multiple techniques
                    
<aside class="notes">
In this final section, we'll look at a complex query that combines several of the advanced techniques we've learned to solve a sophisticated monitoring challenge.

Let's break down this comprehensive query step by step:

1. The innermost part calculates CPU utilization for each instance:
   ```
   sum by (instance) (rate(node_cpu_seconds_total{mode!="idle"}[5m])) / 
   on(instance) (count by(instance) (node_cpu_seconds_total{mode="idle"}))
   ```
   This sums all non-idle CPU time and divides by the number of CPU cores (using the count of idle mode time series as a proxy for core count).

2. The middle part uses a subquery with avg_over_time to calculate the average CPU utilization over the past day, sampled hourly:
   ```
   avg_over_time(
     sum by (instance) (rate(node_cpu_seconds_total{mode!="idle"}[5m])) / 
     on(instance) (count by(instance) (node_cpu_seconds_total{mode="idle"}))[1d:1h]
   )
   ```

3. The comparison operator checks if the current CPU utilization is more than 20% higher than the daily average:
   ```
   current_utilization > 1.2 * historical_average
   ```

4. Finally, topk selects the five instances with the highest values from this comparison:
   ```
   topk(5, comparison_result)
   ```

The overall query identifies the top 5 instances that are experiencing the most unusual increase in CPU utilization compared to their own historical patterns.

This is an excellent example of how combining these advanced features lets you create context-aware monitoring that accounts for:
- Normal resource utilization patterns for each instance
- Historical baselines specific to each instance
- Relative increases rather than absolute thresholds
- Prioritization of the most concerning outliers

When creating complex queries like this, follow these best practices:

1. Build them incrementally, testing each component separately
2. Use comments to document the purpose of each part
3. Consider using recording rules for frequently used subcomponents to improve performance
4. Visualize intermediate results to ensure they're behaving as expected

While complex queries are powerful, remember that readability and maintainability are also important. Sometimes it's better to break very complex logic into multiple recording rules that build on each other, rather than creating one massive query that's difficult to understand and troubleshoot.

With the techniques we've covered in this lab, you now have the tools to create sophisticated, context-aware monitoring solutions that go far beyond simple thresholds.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Summary
                    
                    * Label manipulation functions transform metric labels at query time
                    * The offset modifier enables historical comparisons
                    * Subqueries allow for complex time-based analysis
                    * topk/bottomk functions help identify outliers
                    * The absent function detects missing metrics
                    
                    Next steps:
                    * Apply these techniques to your own metrics
                    * Combine multiple features for powerful insights
                    * Create context-aware alerts using these patterns
                    
<aside class="notes">
In this lab, we've covered five advanced PromQL techniques that significantly expand your monitoring capabilities:

First, we explored label manipulation functions like label_replace and label_join, which allow you to transform, add, and combine metric labels at query time. These functions let you reshape your metric data without changing the underlying collection, giving you more flexibility in how you organize and analyze your metrics.

Next, we discussed the offset modifier, which enables you to look back in time and compare current metrics with historical values. This is essential for trend analysis, detecting anomalies based on historical patterns, and implementing sophisticated alerts that account for normal variations in your metrics.

We then delved into subqueries, one of the most powerful features in PromQL. Subqueries let you apply functions to ranges of already-processed data, enabling complex time-series analysis like moving averages, peak detection, and percentile tracking over extended periods.

The topk and bottomk ranking functions help you identify outliers in your metrics, whether you're looking for the highest resource consumers or the lowest-performing services. These functions are invaluable for focusing attention on the most important metrics among potentially thousands of time series.

Finally, we covered the absent function, which detects when expected metrics are missing entirely. This is crucial for "monitoring your monitoring" and ensuring that your instrumentation itself is working correctly.

The real power of these techniques comes when you combine them to create context-aware monitoring solutions that go beyond simple thresholds. By accounting for historical patterns, relative changes, and expected behaviors, you can build more intelligent alerts that reduce noise and focus on truly significant issues.

As next steps, I encourage you to:
1. Apply these techniques to your own metrics and explore how they can enhance your monitoring
2. Experiment with combining multiple features to create more sophisticated insights
3. Review your existing alerts and consider how these patterns could make them more context-aware and accurate

The advanced techniques we've covered today separate casual PromQL users from true experts. By mastering these concepts, you're well on your way to building world-class monitoring solutions that provide deeper insights into your systems.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    # üåü Great job!
                    
                    Continue to Lab 9: Histograms and Quantiles
                    
                    <small>Navigate: [All Slides](../index.html) | [Next Lab](../09_Histograms_Quantiles/index.html)</small>
                    
<aside class="notes">
Congratulations! You've completed Lab 8 on Advanced PromQL Operations, which has equipped you with powerful techniques for sophisticated data analysis in Prometheus.

In this lab, we explored a variety of advanced PromQL features that extend your monitoring capabilities far beyond basic metrics. You've learned how to transform labels, compare metrics across time periods, analyze complex time-based patterns, identify important outliers, and detect missing metrics.

These techniques represent the transition from basic monitoring to sophisticated observability. With these tools at your disposal, you can create context-aware alerts, perform detailed system analysis, and extract more meaningful insights from your metrics.

As your monitoring needs grow more complex, the advanced techniques covered in this lab will become increasingly valuable. They allow you to adapt to changing requirements without reconfiguring your entire monitoring pipeline, and they help you build more intelligent, less noisy alerting systems.

In the next and final lab, we'll explore histograms and quantiles, which are essential for understanding distributions - particularly for latency and response time metrics. These concepts will complete your PromQL toolkit and allow you to implement sophisticated service level objectives.

Before moving on, try applying some of these advanced techniques to your own metrics. Experiment with label transformations, historical comparisons, or outlier detection to gain new insights into your systems.
</aside>
                </textarea>
            </section>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/notes/notes.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            transition: 'none', // Disable slide animations
            plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
            highlight: {
                highlightOnLoad: true,
                languages: ['promql']
            }
        });
    </script>
    
    <!-- Grafana logo in the corner -->
    <img src="../images/grafana_icon.svg" alt="Grafana" class="grafana-logo">
</body>
</html>
