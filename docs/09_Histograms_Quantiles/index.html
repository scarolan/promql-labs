<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Lab 9: Histograms and Quantiles</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/night.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../common.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section data-markdown>
                <textarea data-template>
                    # Lab 9: Histograms and Quantiles
                    
                    ðŸ“Š Advanced PromQL
                    
                    <small>Navigate: <a href="../index.html">All Slides</a></small>
                    
<aside class="notes">
Welcome to our final lab in the PromQL series, focused on histograms and quantiles. This is one of the most powerful yet challenging aspects of Prometheus.

Histograms are essential for understanding distributions, especially for latency and response time metrics. While previous labs helped us answer questions about "how much" or "how many," histograms help us answer questions about "how fast" and "how consistent."

This lab builds upon all the concepts we've learned so far and will complete our journey by showing how to analyze performance distributions, set meaningful SLOs, and create synthetic histograms for metrics that aren't already in histogram form.

As this is the final lab, encourage students to apply these techniques to their own infrastructure after completing the course. The skills from this lab are particularly valuable for SRE work and service reliability.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Objectives
                    
                    * Understand histogram metrics and their bucket structure
                    * Learn how to use the histogram_quantile function
                    * Analyze latency distributions with bucketing
                    * Calculate SLO-related metrics
                    * Create synthetic histograms for metrics
                    
<aside class="notes">
Let's review our objectives for this lab:

First, we'll understand what histogram metrics are and how they're structured with buckets. Unlike simple counters or gauges, histograms have a special structure designed for distribution analysis.

Second, we'll master the histogram_quantile function, which is the key to extracting percentile data from histogram metrics. This function lets us answer questions like "What's our 95th percentile latency?"

Third, we'll analyze latency distributions with bucketing, seeing how we can track performance across different thresholds.

Fourth, we'll calculate SLO-related metrics like error budgets, which are critical for modern service reliability engineering.

Finally, we'll learn how to create synthetic histograms for metrics that aren't natively captured as histograms. This technique allows you to apply histogram analysis to any metric, significantly expanding your analytical capabilities.

These skills are particularly valuable because they go beyond simple averages, which can be misleading, especially for metrics with non-normal distributions like request latencies.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Histogram Metrics
                    
                    * Special metric type for measuring distributions
                    * Examples: request durations, response sizes
                    * Records observations in cumulative buckets
                    * Bucket label: `le` (less than or equal to)
                    * Each bucket counts observations â‰¤ threshold
                    * Common bucket scheme: `[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]`
                    
<aside class="notes">
Histograms are one of Prometheus's most sophisticated metric types, designed specifically for measuring distributions of values, such as request durations or response sizes.

Unlike counters or gauges, a histogram metric isn't just a single value. Instead, it's a set of counters called "buckets," each associated with an upper bound. Each bucket counts observations that are less than or equal to that bound.

The key to understanding histograms is the "le" label, which stands for "less than or equal to." This label defines the upper bound of each bucket. For example, a bucket with le="0.1" counts all observations that are 0.1 or less.

Buckets are cumulative, which means each bucket includes all observations that would be in the smaller buckets as well. This is an important distinction from regular histograms you might be familiar with from statistics, which typically use non-cumulative bins.

In practice, most Prometheus histograms use a set of predefined buckets with exponentially increasing values, like the example shown. These are optimized for typical web service latency distributions but can be customized for your specific needs.

Prometheus histograms also expose two additional metrics alongside the buckets:
- A _count metric that gives the total count of observations (should match the +Inf bucket)
- A _sum metric that gives the sum of all observed values

Together, these components give us the ability to calculate both simple averages and more complex statistics like percentiles.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Histogram Bucket Structure
                    
                    ```promql
                    # Example: HTTP request duration histogram
                    prometheus_http_request_duration_seconds_bucket{le="0.1"}  -> 1200
                    prometheus_http_request_duration_seconds_bucket{le="0.5"}  -> 1450
                    prometheus_http_request_duration_seconds_bucket{le="1"}    -> 1490
                    prometheus_http_request_duration_seconds_bucket{le="+Inf"} -> 1500
                    
                    # Count and sum metrics also exposed
                    prometheus_http_request_duration_seconds_count -> 1500
                    prometheus_http_request_duration_seconds_sum   -> 750.2
                    ```
                    
                    * Each bucket labeled with threshold
                    * Buckets are cumulative
                    * le="+Inf" bucket contains all observations
                    
<aside class="notes">
This slide illustrates the actual structure of histogram buckets in Prometheus. Let's use an HTTP request duration metric as our example.

Each line represents a different bucket with its threshold value and the cumulative count of observations. Looking at this data:

- 1200 requests took 0.1 seconds or less
- 1450 requests took 0.5 seconds or less (this includes the 1200 that took 0.1s or less)
- 1490 requests took 1 second or less (including all the requests in smaller buckets)
- 1500 requests total, represented by the special "+Inf" (infinity) bucket

The final bucket with le="+Inf" (infinity) is special - it counts all observations regardless of value. This should match the total count metric that's exported alongside the buckets.

When demonstrating this in class, it's helpful to query a real histogram metric and show students the raw data structure before moving on to using functions that operate on this structure.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Exploring Histogram Buckets
                    
                    ```promql
                    # Filter to see specific bucket counts
                    prometheus_http_request_duration_seconds_bucket{handler="/api/v1/query"}
                    ```
                    
                    Returns a list of buckets with cumulative counts:
                    
                    * `le="0.1"`: 1200 requests â‰¤ 100ms
                    * `le="0.5"`: 1450 requests â‰¤ 500ms
                    * `le="1"`: 1490 requests â‰¤ 1s
                    * `le="+Inf"`: 1500 total requests (all)
                    
<aside class="notes">
This slide shows how to explore the bucket structure of a specific histogram metric. Here we're focusing on a specific API endpoint by filtering for the handler="/api/v1/query" label.

When teaching this section, it's valuable to run this query in the Prometheus UI and show the actual results. Let's analyze what the example results tell us:

- 1200 requests took 100ms or less (80% of all requests)
- 1450 requests took 500ms or less (96.7% of all requests)
- 1490 requests took 1 second or less (99.3% of all requests)
- The total number of requests measured was 1500 (represented by the +Inf bucket)

From this data, we can determine that:
- Most requests (80%) are relatively fast (â‰¤100ms)
- A small number (40 requests or ~2.7%) took between 500ms and 1s
- Very few (10 requests or ~0.7%) took more than 1s

This is exactly the kind of distribution insight that's impossible to get from simple averages or even rate calculations. Emphasize to students that this granular understanding of request distribution is crucial for setting realistic SLOs and identifying performance issues.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## The histogram_quantile Function
                    
                    ```promql
                    # Calculate 95th percentile latency
                    histogram_quantile(0.95, sum(rate(prometheus_http_request_duration_seconds_bucket[5m])) by (le))
                    ```
                    
                    * Calculates percentiles from histogram buckets
                    * Result: Value below which 95% of observations fall
                    * Format: `histogram_quantile(Ï† float, b instant-vector)`
                    * Ï†: Quantile between 0 and 1 (e.g., 0.95 for 95th percentile)
                    * b: Vector of bucket counts with the `le` label
                    
<aside class="notes">
The histogram_quantile function is the cornerstone of histogram analysis in Prometheus. This function lets us extract percentile information from histogram bucket data.

Let's break down the example query:

1. The innermost part `rate(prometheus_http_request_duration_seconds_bucket[5m])` calculates the per-second rate of observations for each bucket over a 5-minute window.

2. We then sum these rates and group by the "le" label using `sum(...) by (le)`. This aggregates all time series but maintains the bucket thresholds.

3. Finally, `histogram_quantile(0.95, ...)` calculates the 95th percentile value from these aggregated buckets.

The result is an estimation of the 95th percentile latency - the request duration value below which 95% of all requests complete. For example, if the result is 0.35 seconds, it means 95% of requests complete in 0.35 seconds or less.

It's important to understand that histogram_quantile works by linear interpolation between bucket boundaries. This means the accuracy depends on the bucket definitions - if your buckets are far apart in the range you're interested in, the interpolation becomes less accurate.

Some key points to emphasize:
- Quantile values (Ï†) must be between 0 and 1 (0.5 for median, 0.95 for 95th percentile, etc.)
- The function requires the "le" label to identify bucket thresholds
- The input vector must be in the histogram bucket format
- Percentiles calculated this way are approximations based on bucket boundaries

When teaching this concept, it's helpful to try different quantile values (0.5, 0.9, 0.99) to show how they track different aspects of the distribution.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Aggregating Histogram Buckets
                    
                    ```promql
                    # 95th percentile latency across all handlers
                    histogram_quantile(0.95, 
                      sum(rate(prometheus_http_request_duration_seconds_bucket[5m])) by (le)
                    )
                    
                    # 95th percentile latency by handler
                    histogram_quantile(0.95, 
                      sum(rate(prometheus_http_request_duration_seconds_bucket[5m])) by (handler, le)
                    )
                    ```
                    
                    * Aggregate first, then calculate quantile
                    * Always keep the `le` label when aggregating
                    * Can calculate per-group quantiles by keeping additional labels
                    
<aside class="notes">
This slide demonstrates an important technique when working with histograms: proper aggregation.

The first example calculates the overall 95th percentile across all handlers (endpoints). We first calculate the rate for each bucket, then sum those rates while preserving only the "le" label. This effectively combines the histograms from all handlers into a single histogram, from which we then calculate the 95th percentile.

The second example preserves the "handler" label during aggregation, resulting in a separate 95th percentile calculation for each handler. This is extremely useful for identifying which specific endpoints have performance issues.

There are several important principles to emphasize:

1. When aggregating histogram buckets, always preserve the "le" label - this is essential for the histogram_quantile function to work.

2. Aggregation should happen before applying histogram_quantile, not after. The order of operations matters significantly.

3. You can calculate per-group quantiles by preserving additional labels in the aggregation, like "handler" in our example.

4. Remember that when calculating rates of histogram buckets, we're essentially creating "requests per second in each bucket" metrics, which histogram_quantile can still process correctly.

A common mistake is trying to average percentiles that have already been calculated. Instead, always aggregate the bucket data first, then calculate the percentile. This approach gives the true percentile of the combined distribution, rather than the average of percentiles which can be mathematically incorrect.

This pattern of "aggregate first, calculate percentile second" is crucial for getting accurate distribution metrics in a multi-instance environment.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Percentiles Over Time
                    
                    ```promql
                    # Track 95th percentile latency over time
                    histogram_quantile(0.95, 
                      sum by (le) (
                        rate(prometheus_http_request_duration_seconds_bucket[5m])
                      )
                    )
                    
                    # Multiple percentiles on one graph
                    histogram_quantile(0.5, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))
                    histogram_quantile(0.9, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))
                    histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))
                    histogram_quantile(0.99, sum by (le) (rate(http_request_duration_seconds_bucket[5m])))
                    ```
                    
                    * Track latency distributions over time
                    * Compare different percentiles to understand spread
                    * Useful for identifying gradual performance degradation
                    
<aside class="notes">
This slide demonstrates how to track percentiles over time, which is essential for understanding how your system's performance evolves.

The first query calculates the 95th percentile latency over a rolling 5-minute window, which produces a time series showing how this percentile changes over time. This is far more informative than tracking averages, as it shows what your slowest 5% of users are experiencing.

The second example shows how to track multiple percentiles simultaneously for comparison. By plotting the 50th (median), 90th, 95th, and 99th percentiles together, you can visualize both the typical performance and the long tail of your distribution.

When teaching this section, it's helpful to create a Grafana dashboard with these queries and show how they change over time. Some patterns to point out:

1. A stable median with an increasing 99th percentile often indicates growing inconsistency in performance - most users see acceptable performance, but a growing minority experiences problems.

2. All percentiles increasing together usually suggests a systemic issue affecting all requests.

3. Brief spikes in higher percentiles that don't affect the median typically indicate intermittent issues or batch processes affecting only a subset of requests.

The distance between percentiles also provides insight into the shape of your distribution. A large gap between the 95th and 99th percentiles indicates a long tail of very slow outliers, while percentiles that are close together suggest a more uniform distribution.

These time-based percentile graphs are invaluable for:
- Detecting gradual performance degradation before it becomes critical
- Understanding the impact of changes or deployments on different segments of users
- Establishing realistic SLOs based on observed performance patterns
- Identifying time-based patterns in performance (e.g., daily peaks, weekend variations)
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Tracking Request Rate by Duration
                    
                    ```promql
                    # Requests completing in <= 100ms
                    sum(rate(http_request_duration_seconds_bucket{le="0.1"}[5m]))
                    
                    # Requests completing in 100ms-500ms
                    sum(rate(http_request_duration_seconds_bucket{le="0.5"}[5m])) -
                    sum(rate(http_request_duration_seconds_bucket{le="0.1"}[5m]))
                    
                    # Requests taking > 500ms
                    sum(rate(http_request_duration_seconds_count[5m])) -
                    sum(rate(http_request_duration_seconds_bucket{le="0.5"}[5m]))
                    ```
                    
                    * Track rate of requests in different latency bands
                    * Use subtraction to get non-cumulative ranges
                    * Helps identify changes in distribution
                    
<aside class="notes">
This slide demonstrates a different approach to analyzing latency distributions - tracking the rate of requests falling into specific duration bands.

The first query is straightforward: it shows the rate of requests completing in 100ms or less. Since histogram buckets are cumulative, this directly gives us the rate of "fast" requests.

The second query is more interesting: it calculates the rate of requests completing between 100ms and 500ms. Since buckets are cumulative, we need to subtract the le="0.1" bucket from the le="0.5" bucket to get this specific range.

The third query calculates requests taking more than 500ms by subtracting the le="0.5" bucket from the total count metric. This gives us the rate of "slow" requests.

This approach is particularly valuable for:

1. Creating dashboards that show the distribution of requests across different performance categories (e.g., fast, medium, slow)

2. Setting alerts on specific portions of your distribution rather than on overall percentiles

3. Tracking how changes in your system affect different performance bands

4. Identifying shifts in your latency distribution that might be masked by overall percentiles

For example, you might find that while your 95th percentile remains stable, the rate of requests in the 100ms-500ms band is gradually increasing while those in the <100ms band decrease. This could indicate a subtle performance degradation that wouldn't trigger alerts based on percentiles alone.

When teaching this technique, emphasize that the bands you choose should reflect meaningful thresholds for your specific application. For an API that requires sub-50ms responses, your bands might be <10ms, 10-50ms, and >50ms instead.

This approach complements percentile analysis and provides a different perspective on how your latency distribution changes over time.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Calculating Error Budgets
                    
                    ```promql
                    # Requests exceeding SLO threshold of 300ms
                    sum(rate(http_request_duration_seconds_count[1h])) -
    sum(rate(http_request_duration_seconds_bucket{le="0.3"}[1h]))
                    
                    # Error budget consumed (if SLO is 99.9% under 300ms)
                    100 * (
                      (sum(rate(http_request_duration_seconds_count[1h])) -
                       sum(rate(http_request_duration_seconds_bucket{le="0.3"}[1h]))) /
                      sum(rate(http_request_duration_seconds_count[1h]))
                    ) / 0.001
                    ```
                    
                    * Identify requests exceeding SLO thresholds
                    * Calculate error budget consumption
                    * Track reliability over time
                    
<aside class="notes">
This slide introduces a practical application of histograms for SRE work: calculating error budgets based on latency SLOs.

The first query calculates the rate of requests exceeding our SLO threshold of 300ms. This is done by subtracting the count of requests completing within 300ms from the total count. The result gives us the absolute number of "too slow" requests per second.

The second query is more sophisticated - it calculates the percentage of our error budget consumed. Let's break it down:

1. We calculate the percentage of requests exceeding our threshold:
   ((total_requests - requests_under_300ms) / total_requests) * 100

2. We divide this by our error budget of 0.1% (from the 99.9% SLO), represented as 0.001.
   The result tells us what portion of our error budget we're consuming.

For example:
- If 0.05% of requests exceed 300ms, we're using 50% of our error budget (0.05% / 0.1%)
- If 0.2% of requests exceed 300ms, we're at 200% of our budget (0.2% / 0.1%) - exceeding our SLO

When teaching this concept, emphasize that:

1. The SLO threshold (300ms in this case) should be based on actual user experience requirements, not arbitrary technical metrics.

2. The acceptable error rate (0.1% in this example) should balance reliability goals with engineering costs.

3. Error budgets should be tracked over meaningful time periods - often 30 days rolling windows for customer-facing services.

4. When you exceed your error budget, it signals that it's time to prioritize reliability work over new features.

This approach to SLO tracking is powerful because it translates technical metrics into business-relevant reliability measures and provides a clear signal for when to focus on stability versus feature development.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Building Synthetic Histograms
                    
                    ```promql
                    # Create histogram buckets from a gauge metric
                    count_over_time((memory_usage_bytes < bool 100000000)[1h:1m])
                    count_over_time((memory_usage_bytes < bool 200000000)[1h:1m])
                    count_over_time((memory_usage_bytes < bool 500000000)[1h:1m])
                    count_over_time((memory_usage_bytes < bool 1000000000)[1h:1m])
                    count_over_time((memory_usage_bytes < bool Inf)[1h:1m])
                    ```
                    
                    * Transform gauge metrics into histogram-like structure
                    * Use boolean comparison with count_over_time
                    * Enables histogram_quantile on non-histogram metrics
                    * Requires manual definition of bucket thresholds
                    
<aside class="notes">
This advanced technique demonstrates how to create "synthetic histograms" from regular gauge metrics, allowing you to apply histogram analysis to any metric, not just those explicitly exported as histograms.

The key insight is that by using boolean comparisons with thresholds, we can create the equivalent of histogram buckets from time series samples:

1. The boolean operators (< in this case) convert gauge values into 0 or 1 based on whether they're below the threshold.
2. count_over_time then counts how many samples within our range satisfied each condition.
3. By using multiple thresholds, we build a set of cumulative buckets that mimic a histogram's structure.

Let's interpret our example:
- The first query counts how many 1-minute samples in the past hour had memory usage below 100MB
- The second counts samples below 200MB, and so on
- The final "< Inf" bucket counts all samples, matching the total count in a regular histogram

Once you've created these synthetic buckets, you can use histogram_quantile on them just as you would with real histogram metrics.

Important considerations when teaching this technique:

1. This approach is computationally expensive, so use it judiciously.

2. The accuracy depends on your sampling frequency - more frequent samples give better resolution.

3. You need to carefully choose appropriate bucket thresholds for the metric you're analyzing.

4. To use with histogram_quantile, you need to add a "le" label to each threshold:
   ```
   count_over_time((memory_usage_bytes < bool 100000000)[1h:1m]) * on() group_left(le) label_replace(vector(1), "le", "100000000", "", "")
   ```

This technique is particularly valuable for analyzing the distribution of metrics that aren't exported as histograms but where understanding percentiles would be valuable - like memory usage, queue lengths, or connection counts.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Gotchas and Best Practices
                    
                    * **Bucket definition** affects accuracy - choose appropriate ranges
                    * **Aggregation order** matters - aggregate buckets before calculating quantiles
                    * **Don't average percentiles** - combine bucket data instead
                    * **Beware of quantiles on sparse data** - need sufficient samples
                    * **Recording rules** make histogram queries more efficient
                    
<aside class="notes">
This slide covers some important gotchas and best practices when working with histograms and quantiles.

First, bucket definition significantly affects accuracy. If your buckets are too far apart in the range you're interested in, interpolation becomes less accurate. For example, if you have buckets at 0.1s and 0.5s but most of your traffic falls between those values, your percentiles will be rough approximations. When possible, define custom buckets that provide better resolution in your regions of interest.

Second, aggregation order matters significantly. Always aggregate bucket data first, then calculate quantiles. Never calculate per-instance percentiles and then try to average them - this produces mathematically incorrect results.

Third, never average percentiles from different sources or time periods. It's statistically invalid and will give misleading results. Instead, combine the underlying bucket data and calculate percentiles from that.

Fourth, be cautious with quantiles on sparse data. If you have very few samples in your time range, the percentile calculations become less reliable. As a rule of thumb, you should have at least 100 observations for reasonably accurate percentiles.

Finally, histogram queries with many buckets can be expensive to compute. Consider using recording rules for frequently used histogram_quantile calculations, especially if they're used in dashboards. For example:

```
record: job:http_request_duration_seconds:p95
expr: histogram_quantile(0.95, sum by(job, le) (rate(http_request_duration_seconds_bucket[5m])))
```

This will pre-compute the 95th percentile for each job, making dashboard rendering much faster.

When teaching these concepts, concrete examples of what can go wrong help drive the points home. Consider showing how different bucket definitions affect the same percentile calculation, or how incorrectly averaging percentiles leads to nonsensical results compared to the correct approach.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    ## Summary
                    
                    * Histograms provide distribution analysis for metrics
                    * histogram_quantile calculates percentiles from buckets
                    * Always aggregate buckets before calculating quantiles
                    * Use specific latency bands for targeted analysis
                    * Apply histogram techniques for SLO and error budget tracking
                    * Create synthetic histograms for any metric type
                    
<aside class="notes">
Let's summarize the key points from this lab on histograms and quantiles:

Histograms provide the most comprehensive way to analyze the distribution of metrics like request latencies or response sizes. They overcome the limitations of averages by showing the full shape of your data distribution.

The histogram_quantile function is your primary tool for extracting percentile information from histograms. This lets you track specific percentiles like p50 (median), p95, or p99, which give you a more complete picture of user experience than simple averages.

When working with histograms across multiple instances or time periods, always aggregate the bucket data first, then calculate quantiles. This ensures mathematically correct results.

For more detailed analysis, consider tracking specific latency bands (e.g., <100ms, 100ms-500ms, >500ms) in addition to percentiles. This approach can reveal shifts in your distribution that percentiles might miss.

Histograms are powerful tools for implementing SLO tracking and error budgets. They let you precisely measure what percentage of requests exceed your latency threshold and track how much of your error budget you've consumed.

Finally, even when metrics aren't exported as histograms, you can create synthetic histograms using boolean comparisons and count_over_time. This extends the power of histogram analysis to any metric type.

As this is our final lab, encourage students to apply these techniques to their own infrastructure. Emphasize that mastering histograms and quantiles is what separates basic Prometheus users from those who can derive truly meaningful insights from their metrics.

By combining the concepts from all our labs - from basic query construction through recording rules and now histograms - students now have a comprehensive toolkit for effective monitoring with Prometheus.
</aside>
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    # ðŸŽ“ Congratulations!
                    
                    You've completed the PromQL Labs series!
                    
                    <small>Navigate: [All Slides](../index.html)</small>
                    
<aside class="notes">
Congratulations! You've completed the entire PromQL Labs series! From the basics of CPU metrics to advanced concepts like histograms and quantiles, you've built a comprehensive understanding of how to query, analyze, and monitor metrics with Prometheus.

The skills you've learned in these labs are immediately applicable to real-world monitoring challenges. You now have the tools to:

- Extract meaningful insights from any Prometheus metrics
- Build efficient, reusable queries with recording rules
- Create effective alerts that notify you of actual problems
- Analyze performance distributions with histograms and quantiles
- Implement SLO tracking and error budget monitoring

Remember that mastering PromQL is an ongoing journey. Continue to experiment with these concepts in your own environments, and don't be afraid to push the boundaries of what's possible with these powerful query capabilities.

Thank you for your participation and dedication throughout this course. Your monitoring systems - and the users who depend on them - will benefit greatly from the skills you've developed here.

As you return to your daily work, I encourage you to look for opportunities to apply these techniques to solve real problems. Share what you've learned with colleagues, and continue to build on this foundation as Prometheus and its ecosystem evolve.

Happy monitoring!
</aside>
                </textarea>
            </section>
        </div>
    </div>

    <!-- Include common scripts -->
    <script src="../common-scripts.js"></script>
</body>
</html>
